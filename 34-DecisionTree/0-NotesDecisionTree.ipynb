{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree classifier have two technique \n",
    "\n",
    "1) ID3   2) CART\n",
    "\n",
    "Scalar library follows the Cart technique \n",
    "\n",
    "the difference between them is In cart you can have only binary splits whereas in ID3 you can have more than two splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for checking the purity we will look wheather it is impure split or pure split we have entropy and gini Impurity for figuring out the purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what feature you need to select for split so that we can get leaf node quickly we uses the information gain for that purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In entropy, there can be two output yes or no \n",
    "H(S)=-(P+)log2p - (p-)log2p_\n",
    "here P+ is probability of getting yes output category as output and P- is the probability of getting no output category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the max value in entropy will be 1 range is 0 to 1\n",
    "\n",
    "the max value of gini impurity is 0.5 range is 0 to 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if there are three features f1 f2 f3 which feature will became the parent node and rest be the child node this is decided by information gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "higher the information gain that root feature will be consider as the root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gain(s,f1)=H(s)- summation v belongs to val(value) |Sv|* H(Sv)/|S|\n",
    "\n",
    "here |S| is the total sample  i.e. total no of yes and no in parent node\n",
    " 9 yes  5 NO total =14 sample\n",
    "\n",
    " |Sv| Sample variance i.e. category c1 get how much total yes and no in case of c1 8 and in case of c2 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy vs Gini Impurity\n",
    "when dataset is small we use entropy \n",
    "whereas dataset is large use gini\n",
    "\n",
    "bydefault use gini impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision tree split for numerical feature\n",
    "\n",
    "we will try different threshold to get the best root node to get the leaf node nearest \n",
    "\n",
    "best threshold value is get through information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to millions of dataset the time complexity may get very high in decision tree we have no options  for decision we have to do it there is no optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the overfitting we uses the post prunning and pre prunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Post prunning :- we will first create the complete decision tree and then start cutting it with respect to depth.It is used for smaller dataset\n",
    "for eg if there is node with 9Yes and 2 No the majority is Yes there is no need to go futher instead we will consider it as leaf node "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre prunning :- while creating the decision tree you can play with some parameters like max factores,max depth ,split,etc {Hyperparameter tunning}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In decision regressors,there are continous values unlike categorial values in decision classifier we cannot use gini entropy and information gain here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use variance reduction(regression problem) instead of using information gain\n",
    "\n",
    "in core we are using mean square error of each node and combine them and we will select the node with max variance reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wi is the weight i.e what fraction of values is lying on left side or right side in first case w1 was 1/5 w2 was 3/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after getting the root node with max variance reduction if a test data comes then average of one of the  child node will be the answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
