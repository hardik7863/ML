{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Can we solve sequential data using ANN ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of Sentiment Analysis, we solve sequential data we remove stopwords then apply BOW or any other method which convert text into vectors . The probkem with Ann is that it can convert text to vector but the data is not in sequence due to which the meaning of the sentence is lost then the context is lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of sequential data, it is good to pass one word at a timestamp to neural network to train it properly in ANN we send the complete sentence in one go. Due to which neural network in ann will not be trained efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example :- In languaue translation, one word at a time is translated so that we get the correct context of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rnn has completely similar architecture that of ANN the only difference is that in RNN we have feedback loop the output of the neuron in hidden layer is send to the same neuron and to the neurons present in same hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at timestamp t=1, x11 will be passed then t=2 ,x12 will be passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total 31 trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for converting words into vector in RNN  we can use any of the technique like word 2 vec ,OHE,etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **O.**  usually set zero or some random value depending on the initialization factor. Initially input is not assign and some hidden weights are assigned first with **O.** {zero output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will update the weights in back propogation to get the global minima so that the loss function can be reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to update W.{weight Zero} In the chain rule of derivation, derivation of loss function depends on y^ and y^ depends on old Weight "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to update hidden weights, we use chain of derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DrawBack of ANN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) long term dependency which cannot be captured by RNN:- we have long sentences then there will be more words it will require more time and do not provide more accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Chain rule is so Big which led to gradient vanishing problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.In RNN, derivation of loss function wrt to old hidden weights depends more on the nearest element in case of 50 words there will be t=50 timestamp  then it will depend on the t=50 timestamp and t=1,t=2 will be nearly zero "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nearest element will be responsible for updating the weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution of the problem of simple RNN :-\n",
    "\n",
    "1) Use relu,leaky relu\n",
    "\n",
    "2) LSTM -> Long Short term Memory Rnn\n",
    "\n",
    "3) GRU RNN -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
