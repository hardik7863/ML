{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link for the research paper\n",
    "\n",
    "https://arxiv.org/pdf/1409.0473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S zero is the hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the draw back of encoder decoder architecture is that it is unable to hold the asence of some initial timestamps let says we are at 100 timestamp then it wont be able to hold the t=1,2,3 timestamps \n",
    "\n",
    "some of the changes are made and they come with attention mechanism so that we can overcome the problems of encoder decoder architecture\n",
    "\n",
    "the main idea is to provide more context to decoder to perform the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In attension mechanism architecture, bidirectional lstm rnn is used in the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ai,j is the softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://erdem.pl/2021/05/introduction-to-attention-mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNNProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
