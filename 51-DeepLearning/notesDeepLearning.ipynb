{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the basic unit of ANN and it is the single layer neural network and uses to solve binary classifications\n",
    "\n",
    "it consisit of one neuron which perform two operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights will play very important role because neurons are react based weight like you gripped the hot object then based on weights the neuron will act and remove the hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance of Bias :- The weights are initialise randomly if it initialize with zero then step -1 will become zero and inorder to avoid the entire process to become zero we add bias=1 in it otherwise neuron will get deactivate\n",
    "\n",
    "This bias is consider as noise and in linear regression terms we called it as intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold value is a value above and below of that value output changes like in sigmoid function it is 0.5 and in step function it is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step-2 main aim of Activation function is to transform the output btw some values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron is a linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:-\n",
    "Linear separabale problem can be solved \n",
    "\n",
    "Disadvantages:-\n",
    "It uses feed forward neural network technique which is not very efficient technique because if the output is not correct (error as y and y^ is different )then it need to change the weight and check again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geoffrey Hinton ,father of deep Learning community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiLayer neural Network , there can be any number of hidden layer and any number of neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In forward propogation , we apply step-1 and step-2 in each and every neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a hot object is kept on hand then the weights will get triggered and based on activation function my neuron will get activated then hand will be removed\n",
    "\n",
    "Acitvization function plays important role in activating and deactivating the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end after getting output from last neuron ,loss function is used to calculate the error. If the error is high our main aim is minimize the error then this will be achieved by updating the weights.This can be done by backward propagation.\n",
    "\n",
    "All steps will be repeated untill unless desired output is not achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when two or more than two neuron is involved in hidden layer then matrix multiplication is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Types of Loss Function:-\n",
    "\n",
    "1) For Regression\n",
    "\n",
    "I. MSE\n",
    "\n",
    "II. MAE\n",
    "\n",
    "III.Huber Loss\n",
    "\n",
    "2) For Classification\n",
    "\n",
    "I.Binary Cross Entropy\n",
    "\n",
    "II. Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Backward Propaogation , the weight updation formula is used to update the weight in which slope is calculated by taking differetiation of loss function with respect to weights \n",
    "\n",
    "n is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we plot loss function against weights then a gradient descent is form and we try to reach the global minima.\n",
    "\n",
    "The learning rate decides in how many steps we will reach the global minima Usually it is kept small .If the learning rate has the large value then it will takes larger steps and chances of converging the global minima will not be possible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer is used for forward propogation and backward progoation. there are various optimizers we have studied above is gradient descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vainishing Gradient Problem and Sigmoid function:-\n",
    "In sigmoid function, its range of output is between o to 1 but derivation sigmoid function has the range of output between 0 to 0.25\n",
    "\n",
    "likewise each derivation will be 0-0.25 on multiply all of them you will get the slope of smaller value  then slope * learning rate will be small then Wnew ~= Wold then weights will hardly updated\n",
    "\n",
    "then weights are not updated it means it will not be reaching the global minima this problem is called vanishing gradient descent problem\n",
    "\n",
    "This is happening because of sigmoid function hence we take different activation function\n",
    "\n",
    "Sigmoid function is not good for deep neural network it may work with small neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refer activation function folder then go through activation.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero centered then efficient weight update will happen\n",
    "\n",
    "we perform standard scaler to move the pointes to the center\n",
    "this will help in effecient weight updation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tanh activation function, the range of tanh function is +1 to -1 whereas the range of derivative of tanh function is 0 to 1 but not for middle size neural network but for long deep neural network it may cause vainishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advanatges of tanh is:-\n",
    "1) it is zero centericthen weight updation will be efficient \n",
    "\n",
    "disadvantages:-\n",
    "1) Vanishing gradient problem for deep neural network \n",
    "\n",
    "2) Computaional time is more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Relu Activation Function, its derivation has only two output either 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if there is many neurons and deep neural network we do not use sigmoid or tanh instead we use Relu, Prelu or ELU on hidden layers. \n",
    "\n",
    "If it is the binary classification then use sigmoid function in output layer\n",
    "\n",
    "whereas if it is multiclass classification we use softmax in output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Functtion Vs Cost Function**\n",
    "\n",
    "loss function we pass the each and every data points and calculate the y^ and calculate the loss function and with respect to each and every data point we perform the back propogation\n",
    "\n",
    "we pass all the data point all at once and calculate the cost function and mean square error  we will try to reduce the error through optimizer and weights are updated only at once\n",
    "\n",
    "ek ek data points se khel na hai loss function ek he bar me sare data points se khel na hai cost function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of loss function in regresion:-\n",
    "\n",
    "1) MSE :-it has 1 local or global minima and it is not robust to outlier Because of outlier the cost function will also increases i.e penelizaing (Squaring the error) the error due which are predicted line will get shifted\n",
    "\n",
    "2) Mean absolute error :- it will deal with outlier but takes time in converging\n",
    "\n",
    "IF you have outlier use MAE because if you remove the outlier and then apply Mean square error the data will be loss\n",
    "\n",
    "3) Huber Loss:- Combination of MAE and MSE if error is less than hyperparamter threshold then use  MSE (No outlier) otherwise use Modified MAE formula \n",
    "\n",
    "5) Root mean square error: sq root of MSE\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Advantages of RMS as a Loss Function in Deep Learning\n",
    "\n",
    "1. **Penalizes Large Errors More Heavily**:\n",
    "   - RMS (Root Mean Square) Loss squares the error before averaging, meaning larger errors contribute more to the total loss. This makes it effective at prioritizing minimizing big mistakes.\n",
    "\n",
    "2. **Smooth Optimization**:\n",
    "   - The squared term ensures a smooth gradient, which helps optimization algorithms (like gradient descent) converge steadily.\n",
    "\n",
    "3. **Widely Used**:\n",
    "   - RMS loss (or Mean Squared Error - MSE) is a standard loss function for regression tasks where predicting continuous values is required.\n",
    "\n",
    "4. **Mathematically Intuitive**:\n",
    "   - It’s easy to understand and implement. The squared differences ensure all errors are positive, and taking the mean provides an average sense of error.\n",
    "\n",
    "5. **Balanced Weighting**:\n",
    "   - It treats all errors equally, regardless of whether they are positive or negative.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages of RMS as a Loss Function in Deep Learning\n",
    "\n",
    "1. **Sensitive to Outliers**:\n",
    "   - Squaring errors makes the loss sensitive to large outliers. A few big errors can dominate the loss and skew the model’s focus.\n",
    "\n",
    "2. **Less Robust for Noisy Data**:\n",
    "   - If the dataset has noise or irrelevant data, the model might overfit to these noisy points due to their disproportionate influence.\n",
    "\n",
    "3. **Not Always Ideal for All Problems**:\n",
    "   - RMS loss may not be the best choice for problems where small errors in some predictions are acceptable or where the goal is to minimize relative errors.\n",
    "\n",
    "4. **Slow Convergence**:\n",
    "   - In cases with large outliers, the gradients (used to update model weights) can become very large, slowing down convergence or causing instability.\n",
    "\n",
    "5. **Doesn’t Handle Asymmetric Costs**:\n",
    "   - If overestimating is more problematic than underestimating (or vice versa), RMS cannot differentiate between these types of errors.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary for Beginners\n",
    "- **Good for**: Tasks where predicting accurately on all points is critical, and there’s little noise or outliers in the data.\n",
    "- **Challenges**: Struggles with noisy or imbalanced datasets, as big errors can dominate the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula we use in the binary Cross Entropy is the same formula we use in logistic regression and the sigmoid function is applied on the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorial cross entropy the formula given is the loss function not a learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse Categorial Cross Entropy gives the index of the category with max probability but it do not give any information or probability about other categories whereas Categorial Cross Entropy shows both output category and probability of other categories as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Optimizer** \n",
    "if the loss function is close to zero then predicted output matches the actual output\n",
    "In order to minimize the loss we use optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in gradient descent , 1 epocs =1 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stochastic gradient descent, 1 epocs = n iteration where n is the number of datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mini Batch SDE, noise is reduced but still exit then we use SDE with momentum to smoothen this noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SDE with momentum, we use Beta which will control the smoothening function if the value of beta is high then that point will control the smoothening of curve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Adagrad, we changes the learning rate it is not fixed through out .Initially it is fast and later it becomes slow as it reaches close to the global minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episilon is used in order to avoid the value of learning rate equal to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you are thing why del h is there in learning rate formula then it is not h instead it is the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantage of Adagard is if subtraction term will be come less than Wnew=Wold no updation of weights will take place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAM is the perfect optimizer uses this because it canbe used everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploding Gradient problem :- if the weight initiatilaized is very high due to which learning rate becomes very high due to which it will rarely come to global minima. this situation of exploding gradient problem occurs due to weight initialization technique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly how we face the overfiting in the decision tree similarly we too face overfitting in deeplearning in order to create a generalise model out of it  we use the concept of dropoutlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dropout layers , the dropout ranges from 0 to 1 and different layer has  different dropout probability based on which some of the neuron will not be in use. if the probability is 2/5 where 5 is the total no. of neuron then 2 neuron will be deactivated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we do not apply any dropout while testing data and where as in case  of training data we apply drop out layer. At the end dropout layer is used to generalise the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN:- visual cortex is present in occipital lobe .\n",
    "\n",
    "cerebral cortex is resposible for eye movement.\n",
    "\n",
    "signal first go to the retina then cerebral contrex then to visual cortex present in the occipital lobe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 can send the signal backs to v1 or v3,v4, v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4x4x3 here 3 represent the rgb channels\n",
    "\n",
    "6x6x1 here 1 repesent the greyscale channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CNN, step-1 we perform normalisation i.e converting pixels value within range of 0 to 1\n",
    "\n",
    "This is done by dividing the pixels values by 255.\n",
    "\n",
    "in next step we use filters, which are responsible for taking out information from the image and filter can be of any size \n",
    "\n",
    "CNN operations,\n",
    "we place the filter on our image and multiply with each value which coinciding on one another and calculate the value \n",
    "\n",
    "then we apply stide i.e jump \n",
    "if stride=1 then the filter frame will shift to 1 right \n",
    "after we cannot move to right then come initial place and jump one step down and traverse towards right \n",
    "\n",
    "this how the output is calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will apply the denormalisation of the output \n",
    "\n",
    "remember 0 is black color and 255 is white color\n",
    "\n",
    "then the output we get after denormalization will be able to recognize the vertical edges\n",
    "\n",
    "This is called vertical edge filters \n",
    "\n",
    "Any image we will pass to this filter will give the vertical edges of the  images\n",
    "\n",
    "there are various filters which is used for diff purpose like horizontal edges filter to recognize the horizontal edges ,round edges and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output size of image is calculated by n-f+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding is used prevent the loss of information because we have image of 6x6 we pass to the filter of 3x3 and we get the output of 4x4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In padding we add the layer of cusion you can add whatever layer of cusion depending upon the output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply one layer of padding then 6x6 becomes 8x8 \n",
    "\n",
    "we use the formula  n-f+2p+1=outputsize\n",
    "\n",
    "you can use this formula to find the padding size as per the required size of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what to fill u=in the padding layer blocks there are two ways of doing this\n",
    "\n",
    "1) Zero padding :- filling the padding blocks with zero \n",
    "\n",
    "2) Neighbor Padding :- filling the padding blocks with neighbor values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the operation in Cnn vs ann \n",
    "In cnn,there is image and many filter is applied on the images to get various information \n",
    "\n",
    "when we training the cnn model from scratch we randomly initialise this fiters and this are reintialize / update the value after forward propogation and backward propogation\n",
    "\n",
    "and we apply relu operation on its output becauese we need to update the filter value then we need to apply backpropagation and in backprogation we are applying relu operation then it will find the derivative which can be  0 or 1 which will not cause the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why we apply relu or its variation because we need to find the derivate of the output so we apply the relu on each block of output in order to update the weights in back propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can add as many covolution layer horizontally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling is not part of covolution layer as it is aoplied on output of convolution layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets say we apply 2x2 max pool filter on the output of convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applying the max pooling on the cat images we can extract the cat images very clearly \n",
    "\n",
    "max pooling is extracting the important information from the image\n",
    "\n",
    "this is called location invariant take another example if there is filter for catching  the wheel of the car in the image then this filter can be used to extract information a=about the car in a image this what max pooling do \n",
    "\n",
    "how it works it applies similarly on the output of convoluation layer and get the max value and take stride= 2 i.e it will not repeat the element of the images it will take max jump to find the max value \n",
    "\n",
    "this is how the most intense feature is extract from the image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many architecture you will notice convoluation layer is horizonatlly stack with max pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min pooling , is used for eztracting the min intensity feature\n",
    "\n",
    "and in avg pooling ,we use this for extracting avg intensity features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in flatten layer, we flat the output that we get from the max pooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
