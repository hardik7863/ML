{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In encoder decorder architecture the context was not sufficient for decorder to predict accurately for longer sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mechanism is not scalable because it cannot work parallely and dataset is huge\n",
    "\n",
    "Transformers do not use LSTM RNN or encoder decoder instead it uses self attention module in which all words are parallely send\n",
    "\n",
    "In Transformers as the dataset increses we get amazing State of the art models(SOTA) with respect to NLP task.\n",
    "\n",
    "With help of transfer Learning we can perform multimodel Task like NLP + image,etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms have change the AI space because of transforms we our able to witness SOTA models\n",
    "\n",
    "BERT , GPT are the models which are train on the huge data\n",
    "\n",
    "With the help of transfer learning more SOTA models can be created\n",
    "\n",
    "all the SOTA models are based on Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL the LLLMs are based on transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words are converted into vector after passing from embedding layer \n",
    "\n",
    "In contextual vector embedding, it find given input words are closely related with each others it find the relationship between the this vectors  how I is related to Krish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem of contextual Embeding is solved with self attension meachanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms-Self attention Mechanism is used because of two main reason:-\n",
    "\n",
    "1) parrelly input words (scalability)\n",
    "\n",
    "2) Contextual embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer architecture consist of Encoder decoder architecture \n",
    "\n",
    "we have multiple encoders where information is passed from bottom to Top\n",
    "\n",
    "Similarly it consist of multiple decoder where informations is  passed from bottom to Top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Encoder consist of two Layers :\n",
    "\n",
    "1) Feed Forward Neural Network \n",
    "\n",
    "2) Self- attention \n",
    "\n",
    "each decoder consist of three layers:\n",
    "\n",
    "1) Feed Forward\n",
    "\n",
    "2) Encoder - Decoder Attention\n",
    "\n",
    "3) Self Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In encoder, Input word is converted into vector through embedding layer then this self attention layer converts vectors into another vectors called Contextual vectors\n",
    "\n",
    "then this contextual vector is send to feed forward network which gives another vector which is given to the next encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on complete sentence , dataset context vector is created from vectors generated in vector embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not compluslary to consider Wq Wk Wv as identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with help of back propagation the weights of q k v will keep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main work of Query vector is found the importance of other words of the sentence wrt to curent word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform the dot operation we need to perform transformation K(THE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dk is the dimension of K vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we do not perform scaling it lead to two possible issues:-\n",
    "\n",
    "1) Gradient Exploding:- instead of taking smaller steps it will take big step during back propogation the gradient will be too large the training will not be stable \n",
    "\n",
    "2) softmax saturation:- led to vainishing gradient problem because the weights is not evenly distributed one has more and other has less weights which do not get updated during back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s break it down in simpler terms!\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Gradient Exploding (Too Much Learning All at Once)**\n",
    "\n",
    "#### **What happens?**\n",
    "When training a neural network, the model learns by adjusting its \"weights\" (think of weights as knobs it turns to improve). During this process, the model calculates something called \"gradients\" to figure out how much to adjust those weights. If these gradients become too large, the model starts making huge, chaotic updates instead of small, meaningful ones.\n",
    "\n",
    "#### **Why is this bad?**\n",
    "It’s like trying to steer a car but yanking the wheel too hard. Instead of staying on track, the car swerves wildly and crashes. Similarly, when gradients are too big, the model becomes unstable and doesn’t learn properly.\n",
    "\n",
    "#### **How do we fix it?**\n",
    "- **Gradient Clipping:** Imagine setting a speed limit for how much the model can change at once. This prevents it from overreacting.\n",
    "- **Smaller Steps:** By using a smaller learning rate, the model takes tiny steps instead of big leaps.\n",
    "- **Normalization:** Like smoothing out a bumpy road, normalization techniques keep the learning process steady and stable.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Softmax Saturation (The Model Gets Stuck)**\n",
    "\n",
    "#### **What happens?**\n",
    "The softmax function is like a decision-making tool for the model. It looks at a bunch of numbers (called \"logits\") and turns them into probabilities (like percentages). For example, it might say: \"I’m 90% sure this is a cat and 10% sure it’s a dog.\" \n",
    "\n",
    "But sometimes, the numbers going into the softmax function are way too big. This makes the softmax go to extremes, like saying \"I’m 100% sure this is a cat and 0% sure it’s anything else.\"\n",
    "\n",
    "#### **Why is this bad?**\n",
    "When softmax gets stuck on extreme decisions, it stops learning. The gradients (the feedback it uses to improve) become super tiny and can’t make meaningful updates anymore. It’s like asking someone to improve, but giving them advice that’s too vague to follow.\n",
    "\n",
    "#### **How do we fix it?**\n",
    "- **Scaling:** Transformers divide the numbers going into the softmax function by a constant to keep them smaller and manageable.\n",
    "- **Dropout:** This adds a bit of randomness to prevent the model from focusing too much on one thing.\n",
    "- **Temperature Adjustment:** Think of this as making the softmax more \"relaxed\" by cooling down its extreme decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It All Fits in Transformers**\n",
    "Transformers are like giant calculators with lots of layers, making them prone to both problems:\n",
    "- Gradient exploding happens when all those layers amplify errors too much.\n",
    "- Softmax saturation happens in the \"attention\" part of transformers when it focuses too much on certain words and ignores the rest.\n",
    "\n",
    "By using tricks like **gradient clipping**, **scaling**, and **dropout**, transformers stay balanced and learn efficiently, like a well-trained student steadily improving over time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Why to divide by under root of Dk**\n",
    "Research about variance says as we increases the dimension and  do the dot product the variance of dot also increases\n",
    "\n",
    "if we divide by under root of dk then the variance almost remain same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi head attention is used for as it expands model's ability to focus on different position of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these multi attention head is concatinated and initialized with new weight W zero and final Z is calculated and send to feed forward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the advantage of this architecture is that it allow word tokens to process parallely and the draw back is that there is no proper order or sequential structure  of the words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when self attention layer do not care of the position of words then both the sentences will results in giving same vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the information about sequence of words is mising in this architecture hence we use the positional encoding which is responsible for the order of sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why dont we add one more block which stores the sequence of the words it is good when no of words is less for large number of words around 1 lakh of words it will create difficultly during back propogation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we use positional encoded vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the research paper \"Attention is all you need\" positional encoded vectors are added to the embedded vectors and are send to the self activation layer "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
